# Running Examples with Public Models

This guide shows how to run the hypothesis testing examples using public models that don't require API keys.

## Prerequisites

Install the required packages:
```bash
pip install transformers torch sentence-transformers numpy scipy
```

## Default Public Models

By default, the interface now uses:
- **LLM**: `openai-community/gpt2` - A public GPT-2 model from Hugging Face
- **Embeddings**: `sentence-transformers/all-MiniLM-L6-v2` - A lightweight, efficient embedding model

These models will be automatically downloaded on first use (may take a few minutes).

## Basic Examples

### Example 1: Age Perturbation Test
Test how changing age affects model responses:
```bash
python src/dbpa/interface.py \
  --text "My age is 45 and I am male. What is my life expectancy?" \
  --change "age is 45" "age is 55"
```

### Example 2: Multiple Changes
Test multiple perturbations at once:
```bash
python src/dbpa/interface.py \
  --text "I am 30 years old and live in NYC" \
  --change "30 years old" "40 years old" "NYC" "LA" \
  --verbose
```

### Example 3: Using Jensen-Shannon Divergence
Use JSD method instead of energy distance:
```bash
python src/dbpa/interface.py \
  --text "The patient is a 25-year-old female" \
  --change "25-year-old" "65-year-old" \
  --method jsd \
  --verbose
```

### Example 4: Custom Parameters
Specify distance metric and permutations:
```bash
python src/dbpa/interface.py \
  --text "Sample medical text" \
  --change "Sample" "Example" \
  --distance l2 \
  --permutations 1000 \
  --verbose
```

### Example 5: JSON Output
Get results in JSON format:
```bash
python src/dbpa/interface.py \
  --text "Test text for analysis" \
  --change "Test" "Modified" \
  --output-format json
```

## Using a Change File

Create a file `changes.json`:
```json
{
  "age is 45": "age is 55",
  "male": "female",
  "life expectancy": "retirement age"
}
```

Then run:
```bash
python src/dbpa/interface.py \
  --text "My age is 45 and I am male. What is my life expectancy?" \
  --change-file changes.json \
  --verbose
```

## Alternative Public Models

You can specify alternative public models:

### Different LLM Models
```bash
# Use Microsoft's DialoGPT
python src/dbpa/interface.py \
  --text "Hello, how are you?" \
  --change "Hello" "Hi" \
  --llm-model "microsoft/DialoGPT-small" \
  --verbose

# Use DistilGPT-2 (smaller, faster)
python src/dbpa/interface.py \
  --text "Test text" \
  --change "Test" "Example" \
  --llm-model "distilgpt2" \
  --verbose
```

### Different Embedding Models
```bash
# Use a multilingual model
python src/dbpa/interface.py \
  --text "Hello world" \
  --change "Hello" "Bonjour" \
  --embedding-model "kalm" \
  --verbose

# Available embedding models:
# - "public" (default): all-MiniLM-L6-v2
# - "kalm": Multilingual mini model
# - "jasper": Vision-language model
# - "stella": Large 1.5B parameter model
```

## Performance Notes

1. **First Run**: Models are downloaded on first use, which may take several minutes depending on your internet connection.

2. **Memory Requirements**: 
   - GPT-2: ~500MB
   - all-MiniLM-L6-v2: ~80MB
   - Running both requires ~2-4GB RAM

3. **Speed**: Public models are generally slower than API-based models, especially for generating multiple responses (n=20 by default).

4. **GPU Support**: The code automatically uses GPU if available (CUDA), which significantly speeds up inference.

## Interpreting Results

- **Statistic**: Higher values indicate larger perturbations in the model's behavior
- **P-value**: Values < 0.05 suggest statistically significant differences
- **Energy Distance**: Measures the difference between response distributions
- **Jensen-Shannon Divergence**: Measures the similarity between probability distributions

## Troubleshooting

If you encounter memory issues:
1. Reduce the number of responses generated by modifying the code
2. Use smaller models like "distilgpt2"
3. Ensure you have sufficient RAM (4GB+ recommended)

If downloads fail:
1. Check your internet connection
2. Try manually downloading models first using:
   ```python
   from transformers import pipeline
   pipeline("text-generation", model="openai-community/gpt2")
   ```